{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Score Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to tie together the input generator and neural model scripts to train a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Scripts and Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the desired functions from the input generator and neural model scripts:\n",
    "from imdb_input_generator import *\n",
    "from imdb_neural_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other required libraries:\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras:\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking if weight folder exists:\n",
    "def weight_folder_check(subject, mainfolder, subfolders):\n",
    "    if not os.path.exists(mainfolder):\n",
    "        print 'Creating folders for %s weights...' % (subject)\n",
    "        os.mkdir(mainfolder)\n",
    "        for folder in subfolders:\n",
    "            print 'Creating folder:', folder\n",
    "            os.mkdir(folder)\n",
    "    else:\n",
    "        print 'Folders for %s training weights already exist' % (subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking the latest training epoch number:\n",
    "def epoch_check(count, path, name_template, ext):\n",
    "    for epoch in os.listdir(path):\n",
    "        if len(epoch.split(name_template)) > 1:\n",
    "            print 'Epoch name:', epoch\n",
    "            e = epoch.split(name_template)\n",
    "            e = e[1].split(ext)\n",
    "            e = int(e[0])\n",
    "            if e > count:\n",
    "                count = e\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking latest epoch number for multiple training sets:\n",
    "def epoch_mult_check(folders, name_template, name_ext):\n",
    "    ep_counts = []\n",
    "    counter = -1\n",
    "    for idx, path in enumerate(folders):\n",
    "        train_idx = 't%s_' % (idx + 1)\n",
    "        weight_name_mod = name_template + train_idx\n",
    "        if os.listdir(path) != []:\n",
    "            counter = epoch_check(counter, path, weight_name_mod, name_ext)\n",
    "            print 'Most up-to-date auto-encoder weight file (epoch) for training set %s is indexed: %s' % (idx+1, \n",
    "                                                                                                           counter)\n",
    "            ep_counts.append(counter)\n",
    "        else:\n",
    "            print 'No weight files have been generated yet for training set %s' % (idx+1)\n",
    "            ep_counts.append(counter)\n",
    "    return ep_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate autoencoder weights:\n",
    "def autoencoder_weight_generate(tpaths, name_template, epoch_counts, input_vector_path, model_len, ae_folders, \n",
    "                                weight_name_ext):\n",
    "    # Specify maximum number of epochs to process:\n",
    "    epoch_num = 3\n",
    "    for idx, path in enumerate(tpaths):\n",
    "        print 'Training auto-encoder weights on training set %s' % (idx + 1)\n",
    "        train_idx = 't%s_' % (idx + 1)\n",
    "        weight_name_mod = name_template + train_idx\n",
    "        for num in range(epoch_counts[idx] + 1, epoch_num):\n",
    "            print 'Auto-encoder weight generating on epoch', num\n",
    "            # nb_epoch specifies how many epochs run before saving\n",
    "            # samples_per_epoch specifies # of times to call generator\n",
    "            autoencoder.fit_generator(autoencoder_generator(path, input_vector_path), \n",
    "                                      samples_per_epoch=model_len, nb_epoch=1)\n",
    "            fresh_weight_name = ae_folders[idx] + weight_name_mod + str(num) + weight_name_ext\n",
    "            autoencoder.save_weights(fresh_weight_name)\n",
    "        print 'Weight generation for auto-encoder complete on training set %s' % (idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Folder and File Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT: Please remember to update this below cell to whatever main project directory / training data directory structure you've chosen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory containing data files:\n",
    "parent_path = '/Users/cheng-haotai/Documents/Projects_Data/IMDb_Predictor/'\n",
    "data_name = 'training_data/'\n",
    "data_path = parent_path + data_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below specify file names and directory structures in relation to the parent path defined above. They do not need to be altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training data files that have been saved:\n",
    "train1_name = 'train1_index.pickle'\n",
    "train2_name = 'train2_index.pickle'\n",
    "train3_name = 'train3_index.pickle'\n",
    "train1_path = data_path + train1_name\n",
    "train2_path = data_path + train2_name\n",
    "train3_path = data_path + train3_name\n",
    "tpaths = [train1_path, train2_path, train3_path]\n",
    "\n",
    "train1_folder = 'train1/'\n",
    "train2_folder = 'train2/'\n",
    "train3_folder = 'train3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input data file that has been saved:\n",
    "input_vector_name = 'input_vectors.h5'\n",
    "input_vector_path = data_path + input_vector_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output data file that has been saved:\n",
    "output_vector_name = 'output_data.pickle'\n",
    "output_vector_path = data_path + output_vector_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify test index data files that have been saved:\n",
    "test_name = 'test_index.pickle'\n",
    "test_name_path = data_path + test_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for imdb_regression model:\n",
    "transformed_data_folder = 'transformed_data/'\n",
    "transformed_data_t1 = 'transformed_data_t1.h5'\n",
    "transformed_data_t2 = 'transformed_data_t2.h5'\n",
    "transformed_data_t3 = 'transformed_data_t3.h5'\n",
    "transdata_folder = data_path + transformed_data_folder\n",
    "\n",
    "transdata_path1 = transdata_folder + transformed_data_t1\n",
    "transdata_path2 = transdata_folder + transformed_data_t2\n",
    "transdata_path3 = transdata_folder + transformed_data_t3\n",
    "trans_paths = [transdata_path1, transdata_path2, transdata_path3]\n",
    "\n",
    "transdata1 = 'trans_data_1'\n",
    "transdata2 = 'trans_data_2'\n",
    "transdata3 = 'trans_data_3'\n",
    "transformed_data_dict = [transdata1, transdata2, transdata3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify save parameters for predictions:\n",
    "prediction_data_name = 'score_predictions.csv'\n",
    "prediction_data_path = data_path + prediction_data_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models and Running Predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will be partitioned by function. These functions can be turned on / off by modifying the cell below to \"1\" or \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting flags to turn on/off training segments \n",
    "regression = 1\n",
    "ae = 1\n",
    "test_ae = 1\n",
    "test_regression = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto-Encoder Portion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training weight file details for auto-encoder:\n",
    "weight_folder = data_path + 'autoen_training_weights/'\n",
    "weight_name_template = 'autoen_weights'\n",
    "weight_name_ext = '.h5'\n",
    "ae_train1_folder = weight_folder + train1_folder\n",
    "ae_train2_folder = weight_folder + train2_folder\n",
    "ae_train3_folder = weight_folder + train3_folder\n",
    "ae_folders = [ae_train1_folder, ae_train2_folder, ae_train3_folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get size of neural network input:\n",
    "with h5py.File(input_vector_path, 'r') as input_file:\n",
    "    input_data = input_file['input_dataset'][:]\n",
    "model_len = len(input_data)\n",
    "row_len = len(input_data[0])\n",
    "model_size = (model_len,)  # Tuple of size 1\n",
    "row_size = (row_len,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if weight folder exists and create if it doesn't:\n",
    "weight_folder_check('auto-encoder', weight_folder, ae_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if reduced-dimension dataset folder exists and create if it doesn't:\n",
    "if not os.path.exists(transdata_folder):\n",
    "    print 'Creating folder for reduced-dimensionality datasets...'\n",
    "    os.mkdir(transdata_folder)\n",
    "else:\n",
    "    print 'Folder for reduced-dimensionality datasets already exist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many epochs have been processed:\n",
    "# Counter = -1 if no weights have been generated yet\n",
    "epoch_counts = epoch_mult_check(ae_folders, weight_name_template, weight_name_ext)\n",
    "print epoch_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object of the auto-encoder:\n",
    "print 'Instantiating model for encoder and auto-encoder'\n",
    "encoder, autoencoder = auto_encoder(row_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceeding couple of cells under the auto-encoder portion have created the necessary folder structures and collected the relevant information necessary to train the auto-encoder model and reduce dataset dimensionality. Now, the auto-encoder training will begin. Once weights from auto-encoder training have been generated, the most recent weight file will be loaded into the encoder model for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_ae == 1:\n",
    "    print 'Generating auto-encoder weights...'\n",
    "    # Specify maximum number of epochs to process:\n",
    "    epoch_num = 3\n",
    "    for idx, path in enumerate(tpaths):\n",
    "        print 'Training auto-encoder weights on training set %s' % (idx + 1)\n",
    "        train_idx = 't%s_' % (idx + 1)\n",
    "        weight_name_mod = weight_name_template + train_idx\n",
    "        for num in range(epoch_counts[idx] + 1, epoch_num):\n",
    "            print 'Auto-encoder weight generating on epoch', num\n",
    "            # nb_epoch specifies how many epochs run before saving\n",
    "            # samples_per_epoch specifies # of times to call generator\n",
    "            autoencoder.fit_generator(autoencoder_generator(path, input_vector_path), \n",
    "                                      samples_per_epoch=model_len, nb_epoch=1)\n",
    "            fresh_weight_name = ae_folders[idx] + weight_name_mod + str(num) + weight_name_ext\n",
    "            autoencoder.save_weights(fresh_weight_name)\n",
    "        print 'Weight generation for auto-encoder complete on training set %s' % (idx)\n",
    "#     autoencoder_weight_generate(tpaths, weight_name_template, epoch_counts, input_vector_path, model_len, \n",
    "#                                     ae_folders, weight_name_ext)\n",
    "else:\n",
    "    print 'No further auto-encoder weight generation required'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ae == 1:\n",
    "    for idx, folder in enumerate(ae_folders):\n",
    "        train_idx = 't%s_' % (idx + 1)\n",
    "        weight_name_mod = weight_name_template + train_idx\n",
    "        # Loading weights trained from autoencoder | Encoder can \"see\" weights because by_name = True\n",
    "        print 'Loading latest weight file for training set %s into encoder model...' % (idx + 1)\n",
    "        latest_weight = folder + weight_name_mod + str(epoch_counts[idx]) + weight_name_ext\n",
    "        encoder.load_weights(latest_weight, by_name=True)\n",
    "\n",
    "        # Use auto_encoder to encode data into small dimension (utilizing encoder layer):\n",
    "        if not os.path.exists(trans_paths[idx]):\n",
    "            print('Transforming input data into lower dimensionality...')\n",
    "            transformed_data = []\n",
    "            for row in input_data:\n",
    "                placeholder_input = np.zeros((1, row_len))  # 22000 size placeholder\n",
    "                placeholder_input[0] = row\n",
    "                # Gives result of encoder layer in auto_encoder function\n",
    "                en_predict = encoder.predict(placeholder_input)[0]  # list with a list\n",
    "                transformed_data.append(en_predict)\n",
    "            # Save transformed inputs into h5 file:\n",
    "            print 'Saving transformed data into h5py format...'\n",
    "            transformed_data_file = h5py.File(trans_paths[idx], 'w')\n",
    "            transformed_data_file.create_dataset(transformed_data_dict[idx], data=transformed_data)\n",
    "            print 'Transformed data has been successfully saved for training set %s!' % (idx + 1)\n",
    "        else:\n",
    "            print 'Data has already been transformed in dimensionality for training set %s!' % (idx + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Portion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training weight file details for imdb regression model:\n",
    "imdb_weight_folder = data_path + 'imdb_training_data/'\n",
    "imdb_weight_name_template = 'imdb_weights'\n",
    "imdb_weight_name_ext = '.h5'\n",
    "imdb_train1_folder = imdb_weight_folder + train1_folder\n",
    "imdb_train2_folder = imdb_weight_folder + train2_folder\n",
    "imdb_train3_folder = imdb_weight_folder + train3_folder\n",
    "imdb_folders = [imdb_train1_folder, imdb_train2_folder, imdb_train3_folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out paths of transformed data:\n",
    "trans_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get size of transformed data:\n",
    "print 'Loading transformed data...'\n",
    "loaded_transdata = []\n",
    "for idx, path in enumerate(trans_paths):\n",
    "    with h5py.File(path, 'r') as load_transformed:\n",
    "            loaded_transdata.append(load_transformed[transformed_data_dict[idx]][:])\n",
    "# Initialize size values as zeros:\n",
    "transformed_len = 0\n",
    "trans_row_len = 0\n",
    "trans_row_size = 0\n",
    "for data in loaded_transdata:\n",
    "    transformed_len += len(data)\n",
    "    trans_row_len += len(data[0])\n",
    "# Get 'averaged' data size:\n",
    "transformed_len = transformed_len / len(loaded_transdata)\n",
    "trans_row_len = trans_row_len / len(loaded_transdata)\n",
    "trans_row_size = (trans_row_len,)  # Length 1 tuple (10000 value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if weight folder exists and create if it doesn't:\n",
    "weight_folder_check('IMDb regression', imdb_weight_folder, imdb_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many epochs have been processed:\n",
    "# Counter = -1 if no weights have been generated yet\n",
    "imdb_epoch_counts = epoch_mult_check(imdb_folders, imdb_weight_name_template, weight_name_ext)\n",
    "print imdb_epoch_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regression == 1:\n",
    "    \n",
    "    # Create model object of imdb regression model:\n",
    "    imdb_reg = imdb_regression(trans_row_size)\n",
    "    \n",
    "    # Define name of imdb weight file:\n",
    "    imdb_weight_file_name = imdb_weight_folder + imdb_weight_name_template + str(imdb_counter) + imdb_weight_name_ext\n",
    "    # Load imdb weight file if it exists:\n",
    "    if os.path.exists(imdb_weight_file_name):\n",
    "        print 'Latest imdb weight file already exists! Loading...'\n",
    "        # by_name allows for old weight files to be used with new models with new structures\n",
    "        imdb_reg.load_weights(imdb_weight_file_name, by_name=True)\n",
    "    else:\n",
    "        print 'No imdb weights have been generated yet'\n",
    "    \n",
    "    if test_regression == 1:\n",
    "        # Specify maximum number of epochs to process in imdb weight training:\n",
    "        imdb_epoch_num = 1000\n",
    "        for num in range(imdb_counter + 1, imdb_epoch_num):\n",
    "            print('IMDb regression weight training on epoch:', num)\n",
    "            imdb_reg.fit_generator(input_generator(train1_path, output_vector_path, transformed_data_path),\n",
    "                                   samples_per_epoch = transformed_len, nb_epoch=1)\n",
    "            fresh_imdb_weight_name = imdb_weight_folder + imdb_weight_name_template + str(num) + imdb_weight_name_ext\n",
    "            imdb_reg.save_weights(fresh_imdb_weight_name)\n",
    "            \n",
    "    else:\n",
    "        # Load latest weights for imdb model:\n",
    "        imdb_latest_weight = imdb_weight_folder + imdb_weight_name_template + str(imdb_counter) + imdb_weight_name_ext\n",
    "        imdb_reg.load_weights(imdb_latest_weight, by_name=True)\n",
    "\n",
    "        # Run predictions on test data:\n",
    "        # Load test index file and output data file:\n",
    "        with open(test_name_path, 'rb') as test_set:\n",
    "            test_data = pkl.load(test_set)\n",
    "        with open(output_vector_path, 'rb') as output_set:\n",
    "            output_data = pkl.load(output_set)\n",
    "\n",
    "        # Check whether or not predictions have been run:\n",
    "        if not os.path.exists(prediction_data_path):\n",
    "            print('Running predictions on test data...')\n",
    "            prediction_vec = []\n",
    "            score_vec = []\n",
    "            # Train on test data:\n",
    "            for index in test_data:\n",
    "                # Pull necessary data using index in test data:\n",
    "                test_row = transformed_data[index]\n",
    "                score_val = output_data[index]\n",
    "                # Populate placeholder vector:\n",
    "                placeholder_input = np.zeros((1, trans_row_len))  # 10000 placeholder\n",
    "                placeholder_input[0] = test_row\n",
    "                imdb_predict = imdb_reg.predict(placeholder_input)\n",
    "                prediction_vec.append(imdb_predict)\n",
    "                score_vec.append(score_val)\n",
    "            \n",
    "            print 'Predictions have completed! Proceeding to save data...'\n",
    "            final_results = pd.DataFrame()\n",
    "            final_results['Real_Score'] = score_vec\n",
    "            final_results['Predicted_Score'] = prediction_vec\n",
    "            final_results.to_csv(prediction_data_path)\n",
    "            print 'Prediction data has been saved!'\n",
    "        else:\n",
    "            print 'Predictions for IMDb scores have already been run!'\n",
    "            final_results = pd.read_csv(prediction_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
