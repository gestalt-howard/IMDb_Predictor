{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to tie together the input generator and neural model scripts to train a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the desired functions from the input generator and neural model scripts:\n",
    "from imdb_input_generator import *\n",
    "from imdb_neural_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other required libraries:\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras:\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking the latest training epoch number:\n",
    "def epoch_check(count, path, name_template, ext):\n",
    "    for epoch in os.listdir(path):\n",
    "        e = epoch.split(name_template)\n",
    "        e = e[1].split(ext)\n",
    "        e = int(e[0])\n",
    "        if e > count:\n",
    "            count = e\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory containing data files:\n",
    "parent_path = '/Users/cheng-haotai/Documents/Projects_Data/IMDb_Predictor/'\n",
    "data_name = 'training_data/'\n",
    "data_path = parent_path + data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training data files that have been saved:\n",
    "train1_name = 'train1_index.pickle'\n",
    "train2_name = 'train2_index.pickle'\n",
    "train3_name = 'train3_index.pickle'\n",
    "train1_path = data_path + train1_name\n",
    "train2_path = data_path + train2_name\n",
    "train3_path = data_path + train3_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input data file that has been saved:\n",
    "input_vector_name = 'input_vectors.h5'\n",
    "input_vector_path = data_path + input_vector_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output data file that has been saved:\n",
    "output_vector_name = 'output_data.pickle'\n",
    "output_vector_path = data_path + output_vector_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify test index data files that have been saved:\n",
    "test_name = 'test_index.pickle'\n",
    "test_name_path = data_path + test_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training weight file details for auto-encoder:\n",
    "weight_folder = data_path + 'autoen_training_weights/'\n",
    "weight_name_template = 'autoen_weights'\n",
    "weight_name_ext = '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training weight file details for imdb regression model:\n",
    "imdb_weight_folder = data_path + 'imdb_training_data/'\n",
    "imdb_weight_name_template = 'imdb_weights'\n",
    "imdb_weight_name_ext = '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for imdb_regression model:\n",
    "transformed_data_name = 'transformed_data.h5'\n",
    "transformed_data_path = data_path + transformed_data_name\n",
    "transformed_data_dict = 'transformed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify save parameters for predictions:\n",
    "prediction_data_name = 'score_predictions.csv'\n",
    "prediction_data_path = data_path + prediction_data_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will be partitioned by function. These functions can be turned on / off by modifying the cell below to \"1\" or \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting flags to turn on/off training \n",
    "regression = 1\n",
    "ae = 0\n",
    "test_ae = 0  # Allows for early termination of autoencoder training process\n",
    "test_regression = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ae == 1:\n",
    "    # Auto-Encoder Portion:\n",
    "    print('Starting auto-encoder portion:')\n",
    "    # Get size of network input:\n",
    "    with h5py.File(input_vector_path, 'r') as input_file:\n",
    "        input_data = input_file['input_dataset'][:]\n",
    "    \n",
    "    model_len = len(input_data)\n",
    "    row_len = len(input_data[0])\n",
    "    model_size = (model_len,)  # Tuple of size 1\n",
    "    row_size = (row_len,)\n",
    "    print('Tuple to input into auto encoder function has size:', model_size)\n",
    "    # Create a model object of the auto-encoder:\n",
    "    print('Generating model for encoder and auto-encoder:')\n",
    "    encoder, autoencoder = auto_encoder(model_size)\n",
    "    \n",
    "    # Check if weight folder exists and create if it doesn't:\n",
    "    # Typically all training weights for a model will live in one folder\n",
    "    if not os.path.exists(weight_folder):\n",
    "        print('Creating folder for auto-encoder weights...')\n",
    "        os.mkdir(weight_folder)\n",
    "    else:\n",
    "        print('Folder for auto-encoder weights already exists!')\n",
    "        \n",
    "    # If the weight folder exists, check how many epochs have been processed:\n",
    "    counter = -1\n",
    "    if os.path.exists(weight_folder):\n",
    "        # Finding the number assigned to most recent epoch\n",
    "        counter = epoch_check(counter, weight_folder, weight_name_template, weight_name_ext)\n",
    "        print 'Most up-to-date auto-encoder weight file (epoch) is indexed:', counter\n",
    "    else:\n",
    "        # Set counter to -1 if no epochs have been processed\n",
    "        counter = -1\n",
    "        print('No epochs for auto-encoder have run yet')\n",
    "    \n",
    "    # Check if the latest weight file can be loaded:\n",
    "    weight_file_name = weight_folder + weight_name_template + str(counter) + weight_name_ext\n",
    "    if os.path.exists(weight_file_name):\n",
    "        print('Latest auto-encoder weight file already exists! Loading...')\n",
    "        # by_name allows for old weight files to be used with new models with new structures\n",
    "        autoencoder.load_weights(weight_file_name, by_name=True)\n",
    "    else:\n",
    "        print('No auto-encoder weights have been generated yet')\n",
    "    \n",
    "    if test_ae == 0:\n",
    "        print 'Generating weights from latest weight file...'\n",
    "        # Specify maximum number of epochs to process:\n",
    "        epoch_num = 1000\n",
    "        for num in range(counter + 1, epoch_num):\n",
    "            print('Auto-encoder weight generating on epoch', num)\n",
    "            # Train (implies updating error) and generate weights:\n",
    "            # nb_epoch specifies how many epochs run before saving (1 in this case)\n",
    "            # samples_per_epoch specifies # of times to call generator\n",
    "            autoencoder.fit_generator(autoencoder_generator(train1_path, input_vector_path), samples_per_epoch=model_len, nb_epoch=1)\n",
    "            fresh_weight_name = weight_folder + weight_name_template + str(num) + weight_name_ext\n",
    "            autoencoder.save_weights(fresh_weight_name)\n",
    "    else:\n",
    "        print 'No further weight generation required'\n",
    "        # Assuming that all epochs have been processed and no need to check last epoch:\n",
    "        # Each weight file is the next progression (weight update) of the previous weight file\n",
    "        # Loading weights trained from autoencoder (shared weights)\n",
    "        # Encoder can \"see\" weights because by_name = True\n",
    "        print('Loading latest weight file into encoder model...')\n",
    "        latest_weight = weight_folder + weight_name_template + str(counter) + weight_name_ext\n",
    "        encoder.load_weights(latest_weight, by_name=True)\n",
    "\n",
    "        # Use auto_encoder to encode data into small dimension (utilizing encoder layer):\n",
    "        if not os.path.exists(transformed_data_path):\n",
    "            print('Transforming input data into lower dimensionality...')\n",
    "            transformed_data = []\n",
    "            for row in input_data:\n",
    "                placeholder_input = np.zeros((1, row_len))  # 22000 size placeholder\n",
    "                placeholder_input[0] = row\n",
    "                # Gives result of encoder layer in auto_encoder function\n",
    "                # Predict doesn't update error - simply multiplies input by weights\n",
    "                en_predict = encoder.predict(placeholder_input)[0]  # list with 1 list in it & needed to take the first element\n",
    "                transformed_data.append(en_predict)\n",
    "            # Save transformed inputs into h5 file:\n",
    "            print('Saving transformed data into h5py format...')\n",
    "            transformed_data_file = h5py.File(transformed_data_path, 'w')  # Aaron: wb turned to w\n",
    "            transformed_data_file.create_dataset(transformed_data_dict, data=transformed_data)\n",
    "        else:\n",
    "            print('Data has already been transformed in dimensionality!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regression == 1:\n",
    "    # Post Auto-Encoder Portion:\n",
    "    print 'Progressing onto regression model...'\n",
    "\n",
    "    print 'Loading transformed data...'\n",
    "    with h5py.File(transformed_data_path, 'r') as load_transformed:\n",
    "        transformed_data = load_transformed['transformed_data'][:]\n",
    "\n",
    "    # Find length of input data from transformed data:\n",
    "    transformed_len = len(transformed_data)\n",
    "    trans_row_len = len(transformed_data[0])\n",
    "    trans_row_size = (trans_row_len,)  # Length 1 tuple (10000 value)\n",
    "    # Create model object of imdb regression model:\n",
    "    imdb_reg = imdb_regression(transformed_size)\n",
    "    \n",
    "    # Check if weight folder exists and create if it doesn't:\n",
    "    # Typically all training weights for a model will live in one folder\n",
    "    if not os.path.exists(imdb_weight_folder):\n",
    "        os.mkdir(imdb_weight_folder)\n",
    "\n",
    "    # If the weight folder exists, check how many epochs have been processed:\n",
    "    imdb_counter = -1\n",
    "    if os.path.exists(imdb_weight_folder):\n",
    "        # Finding the number assigned to most recent epoch\n",
    "        imdb_counter = epoch_check(imdb_counter, imdb_weight_folder, imdb_weight_name_template, imdb_weight_name_ext)\n",
    "        print 'Latest IMDb regression weight file (epoch) is indexed', imdb_counter\n",
    "    else:\n",
    "        imdb_counter = -1\n",
    "    \n",
    "    # Define name of imdb weight file:\n",
    "    imdb_weight_file_name = imdb_weight_folder + imdb_weight_name_template + str(imdb_counter) + imdb_weight_name_ext\n",
    "    # Load imdb weight file if it exists:\n",
    "    if os.path.exists(imdb_weight_file_name):\n",
    "        print 'Latest imdb weight file already exists! Loading...'\n",
    "        # by_name allows for old weight files to be used with new models with new structures\n",
    "        imdb_reg.load_weights(imdb_weight_file_name, by_name=True)\n",
    "    else:\n",
    "        print 'No imdb weights have been generated yet'\n",
    "    \n",
    "    if test_regression == 0:\n",
    "        # Specify maximum number of epochs to process in imdb weight training:\n",
    "        imdb_epoch_num = 1000\n",
    "        for num in range(imdb_counter + 1, imdb_epoch_num):\n",
    "            print('IMDb regression weight training on epoch:', num)\n",
    "            imdb_reg.fit_generator(input_generator(train1_path, output_vector_path, transformed_data_path),\n",
    "                                   samples_per_epoch = transformed_len,\n",
    "                                   nb_epoch=1)\n",
    "            fresh_imdb_weight_name = imdb_weight_folder + imdb_weight_name_template + str(num) + imdb_weight_name_ext\n",
    "            imdb_reg.save_weights(fresh_imdb_weight_name)\n",
    "            \n",
    "    else:\n",
    "        # Load latest weights for imdb model:\n",
    "        imdb_latest_weight = imdb_weight_folder + imdb_weight_name_template + str(imdb_counter) + imdb_weight_name_ext\n",
    "        imdb_reg.load_weights(imdb_latest_weight, by_name=True)\n",
    "\n",
    "        # Run predictions on test data:\n",
    "        # Load test index file and output data file:\n",
    "        with open(test_name_path, 'rb') as test_set:\n",
    "            test_data = pkl.load(test_set)\n",
    "        with open(output_vector_path, 'rb') as output_set:\n",
    "            output_data = pkl.load(output_set)\n",
    "\n",
    "        # Check whether or not predictions have been run:\n",
    "        if not os.path.exists(prediction_data_path):\n",
    "            print('Running predictions on test data...')\n",
    "            prediction_vec = []\n",
    "            score_vec = []\n",
    "            # Train on test data:\n",
    "            for index in test_data:\n",
    "                # Pull necessary data using index in test data:\n",
    "                test_row = transformed_data[index]\n",
    "                score_val = output_data[index]\n",
    "                # Populate placeholder vector:\n",
    "                placeholder_input = np.zeros((1, trans_row_len))  # 10000 placeholder\n",
    "                placeholder_input[0] = test_row\n",
    "                imdb_predict = imdb_reg.predict(placeholder_input)\n",
    "                prediction_vec.append(imdb_predict)\n",
    "                score_vec.append(score_val)\n",
    "            \n",
    "            print('Predictions have completed! Proceeding to save data...')\n",
    "            final_results = pd.DataFrame()\n",
    "            final_results['Real_Score'] = score_vec\n",
    "            final_results['Predicted_Score'] = prediction_vec\n",
    "            final_results.to_csv(prediction_data_path)\n",
    "        else:\n",
    "            print('Predictions for IMDb scores have already been run!')\n",
    "            # final_results = pd.read_csv(prediction_data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
