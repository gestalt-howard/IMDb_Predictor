{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Score Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to tie together the input generator and neural model scripts to train a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Scripts and Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the desired functions from the input generator and neural model scripts:\n",
    "from imdb_input_generator import *\n",
    "from imdb_neural_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other required libraries:\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras:\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking if weight folder exists:\n",
    "def weight_folder_check(subject, mainfolder, subfolders):\n",
    "    if not os.path.exists(mainfolder):\n",
    "        print 'Creating folders for %s weights...' % (subject)\n",
    "        os.mkdir(mainfolder)\n",
    "        for folder in subfolders:\n",
    "            print 'Creating folder:', folder\n",
    "            os.mkdir(folder)\n",
    "    else:\n",
    "        print 'Folders for %s training weights already exist' % (subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking the latest training epoch number:\n",
    "def epoch_check(count, path, name_template, ext):\n",
    "    for epoch in os.listdir(path):\n",
    "        if len(epoch.split(name_template)) > 1:\n",
    "            print 'Epoch name:', epoch\n",
    "            e = epoch.split(name_template)\n",
    "            e = e[1].split(ext)\n",
    "            e = int(e[0])\n",
    "            if e > count:\n",
    "                count = e\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking latest epoch number for multiple training sets:\n",
    "def epoch_mult_check(folders, name_template, name_ext):\n",
    "    ep_counts = []\n",
    "    counter = -1\n",
    "    for idx, path in enumerate(folders):\n",
    "        train_idx = 't%s_' % (idx + 1)\n",
    "        weight_name_mod = name_template + train_idx\n",
    "        if os.listdir(path) != []:\n",
    "            counter = epoch_check(counter, path, weight_name_mod, name_ext)\n",
    "            print 'Most up-to-date auto-encoder weight file (epoch) for training set %s is indexed: %s\\n' % (idx+1, \n",
    "                                                                                                           counter)\n",
    "            ep_counts.append(counter)\n",
    "        else:\n",
    "            print 'No weight files have been generated yet for training set %s' % (idx+1)\n",
    "            ep_counts.append(counter)\n",
    "    return ep_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Folder and File Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT: Please remember to update this below cell to whatever main project directory / training data directory structure you've chosen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory containing data files:\n",
    "parent_path = '/Users/cheng-haotai/Documents/Projects_Data/IMDb_Predictor/'\n",
    "data_name = 'training_data/'\n",
    "data_path = parent_path + data_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below specify file names and directory structures in relation to the parent path defined above. They do not need to be altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training data files that have been saved:\n",
    "train1_name = 'train1_index.pickle'\n",
    "train2_name = 'train2_index.pickle'\n",
    "train3_name = 'train3_index.pickle'\n",
    "train1_path = data_path + train1_name\n",
    "train2_path = data_path + train2_name\n",
    "train3_path = data_path + train3_name\n",
    "tpaths = [train1_path, train2_path, train3_path]\n",
    "\n",
    "train1_folder = 'train1/'\n",
    "train2_folder = 'train2/'\n",
    "train3_folder = 'train3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input data file that has been saved:\n",
    "input_vector_name = 'input_vectors.h5'\n",
    "input_vector_path = data_path + input_vector_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output data file that has been saved:\n",
    "output_vector_name = 'output_data.pickle'\n",
    "output_vector_path = data_path + output_vector_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify test index data files that have been saved:\n",
    "test_name = 'test_index.pickle'\n",
    "test_name_path = data_path + test_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for imdb_regression model:\n",
    "transformed_data_folder = 'transformed_data/'\n",
    "transformed_data_t1 = 'transformed_data_t1.h5'\n",
    "transformed_data_t2 = 'transformed_data_t2.h5'\n",
    "transformed_data_t3 = 'transformed_data_t3.h5'\n",
    "transdata_folder = data_path + transformed_data_folder\n",
    "\n",
    "transdata_path1 = transdata_folder + transformed_data_t1\n",
    "transdata_path2 = transdata_folder + transformed_data_t2\n",
    "transdata_path3 = transdata_folder + transformed_data_t3\n",
    "trans_paths = [transdata_path1, transdata_path2, transdata_path3]\n",
    "\n",
    "transdata1 = 'trans_data_1'\n",
    "transdata2 = 'trans_data_2'\n",
    "transdata3 = 'trans_data_3'\n",
    "transformed_data_dict = [transdata1, transdata2, transdata3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify save parameters for predictions:\n",
    "prediction_data_name = 'score_predictions.csv'\n",
    "prediction_data_path = data_path + prediction_data_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models and Running Predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will be partitioned by function. These functions can be turned on / off by modifying the cell below to \"1\" or \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting flags to turn on/off training segments \n",
    "regression = 1\n",
    "ae = 1\n",
    "test_ae = 1\n",
    "test_regression = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto-Encoder Portion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training weight file details for auto-encoder:\n",
    "weight_folder = data_path + 'autoen_training_weights/'\n",
    "weight_name_template = 'autoen_weights'\n",
    "weight_name_ext = '.h5'\n",
    "ae_train1_folder = weight_folder + train1_folder\n",
    "ae_train2_folder = weight_folder + train2_folder\n",
    "ae_train3_folder = weight_folder + train3_folder\n",
    "ae_folders = [ae_train1_folder, ae_train2_folder, ae_train3_folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get size of neural network input:\n",
    "with h5py.File(input_vector_path, 'r') as input_file:\n",
    "    input_data = input_file['input_dataset'][:]\n",
    "model_len = len(input_data)\n",
    "row_len = len(input_data[0])\n",
    "model_size = (model_len,)  # Tuple of size 1\n",
    "row_size = (row_len,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if weight folder exists and create if it doesn't:\n",
    "weight_folder_check('auto-encoder', weight_folder, ae_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if reduced-dimension dataset folder exists and create if it doesn't:\n",
    "if not os.path.exists(transdata_folder):\n",
    "    print 'Creating folder for reduced-dimensionality datasets...'\n",
    "    os.mkdir(transdata_folder)\n",
    "else:\n",
    "    print 'Folder for reduced-dimensionality datasets already exist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many epochs have been processed:\n",
    "# Counter = -1 if no weights have been generated yet\n",
    "epoch_counts = epoch_mult_check(ae_folders, weight_name_template, weight_name_ext)\n",
    "print epoch_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object of the auto-encoder:\n",
    "print 'Instantiating model for encoder and auto-encoder'\n",
    "encoder, autoencoder = auto_encoder(row_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceeding couple of cells under the auto-encoder portion have created the necessary folder structures and collected the relevant information necessary to train the auto-encoder model and reduce dataset dimensionality. Now, the auto-encoder training will begin. Once weights from auto-encoder training have been generated, the most recent weight file will be loaded into the encoder model for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_ae == 1:\n",
    "    print 'Generating auto-encoder weights...'\n",
    "    # Specify maximum number of epochs to process:\n",
    "    epoch_num = 3\n",
    "    for idx, path in enumerate(tpaths):\n",
    "        print 'Training auto-encoder weights on training set %s' % (idx + 1)\n",
    "        train_idx = 't%s_' % (idx + 1)\n",
    "        weight_name_mod = weight_name_template + train_idx\n",
    "        for num in range(epoch_counts[idx] + 1, epoch_num):\n",
    "            print 'Auto-encoder weight generating on epoch', num\n",
    "            # nb_epoch specifies how many epochs run before saving\n",
    "            # samples_per_epoch specifies # of times to call generator\n",
    "            autoencoder.fit_generator(autoencoder_generator(path, input_vector_path), \n",
    "                                      samples_per_epoch=model_len, nb_epoch=1)\n",
    "            fresh_weight_name = ae_folders[idx] + weight_name_mod + str(num) + weight_name_ext\n",
    "            autoencoder.save_weights(fresh_weight_name)\n",
    "        print 'Weight generation for auto-encoder complete on training set %s\\n' % (idx + 1)\n",
    "else:\n",
    "    print 'No further auto-encoder weight generation required'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many epochs have been processed:\n",
    "epoch_counts = epoch_mult_check(ae_folders, weight_name_template, weight_name_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ae == 1:\n",
    "    for idx, folder in enumerate(ae_folders):\n",
    "        train_idx = 't%s_' % (idx + 1)\n",
    "        weight_name_mod = weight_name_template + train_idx\n",
    "        # Loading weights trained from autoencoder | Encoder can \"see\" weights because by_name = True\n",
    "        print 'Loading latest weight file for training set %s into encoder model...' % (idx + 1)\n",
    "        latest_weight = folder + weight_name_mod + str(epoch_counts[idx]) + weight_name_ext\n",
    "        encoder.load_weights(latest_weight, by_name=True)\n",
    "\n",
    "        # Use auto_encoder to encode data into small dimension (utilizing encoder layer):\n",
    "        if not os.path.exists(trans_paths[idx]):\n",
    "            print('Transforming input data into lower dimensionality...')\n",
    "            transformed_data = []\n",
    "            for row in input_data:\n",
    "                placeholder_input = np.zeros((1, row_len))  # 22000 size placeholder\n",
    "                placeholder_input[0] = row\n",
    "                # Gives result of encoder layer in auto_encoder function\n",
    "                en_predict = encoder.predict(placeholder_input)[0]  # list with a list\n",
    "                transformed_data.append(en_predict)\n",
    "            # Save transformed inputs into h5 file:\n",
    "            print 'Saving transformed data into h5py format...'\n",
    "            transformed_data_file = h5py.File(trans_paths[idx], 'w')\n",
    "            transformed_data_file.create_dataset(transformed_data_dict[idx], data=transformed_data)\n",
    "            print 'Transformed data has been successfully saved for training set %s!\\n' % (idx + 1)\n",
    "        else:\n",
    "            print 'Data has already been transformed in dimensionality for training set %s!' % (idx + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Portion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training weight file details for imdb regression model:\n",
    "imdb_weight_folder = data_path + 'imdb_training_data/'\n",
    "imdb_weight_name_template = 'imdb_weights'\n",
    "imdb_weight_name_ext = '.h5'\n",
    "imdb_train1_folder = imdb_weight_folder + train1_folder\n",
    "imdb_train2_folder = imdb_weight_folder + train2_folder\n",
    "imdb_train3_folder = imdb_weight_folder + train3_folder\n",
    "imdb_folders = [imdb_train1_folder, imdb_train2_folder, imdb_train3_folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out paths of transformed data:\n",
    "trans_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get size of transformed data:\n",
    "print 'Loading transformed data...'\n",
    "loaded_transdata = []\n",
    "for idx, path in enumerate(trans_paths):\n",
    "    with h5py.File(path, 'r') as load_transformed:\n",
    "            loaded_transdata.append(load_transformed[transformed_data_dict[idx]][:])\n",
    "# Initialize size values as zeros:\n",
    "transformed_len = 0\n",
    "trans_row_len = 0\n",
    "trans_row_size = 0\n",
    "for data in loaded_transdata:\n",
    "    transformed_len += len(data)\n",
    "    trans_row_len += len(data[0])\n",
    "# Get 'averaged' data size:\n",
    "transformed_len = transformed_len / len(loaded_transdata)\n",
    "trans_row_len = trans_row_len / len(loaded_transdata)\n",
    "trans_row_size = (trans_row_len,)  # Length 1 tuple (10000 value)\n",
    "print 'Length of transformed dataset:', transformed_len\n",
    "print 'Lenght of a row of the transformed dataset', trans_row_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if weight folder exists and create if it doesn't:\n",
    "weight_folder_check('IMDb regression', imdb_weight_folder, imdb_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many epochs have been processed:\n",
    "# Counter = -1 if no weights have been generated yet\n",
    "imdb_epoch_counts = epoch_mult_check(imdb_folders, imdb_weight_name_template, weight_name_ext)\n",
    "print imdb_epoch_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object of imdb regression model:\n",
    "imdb_reg = imdb_regression(trans_row_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_regression == 1:\n",
    "    print 'Generating imdb-regression weights...'\n",
    "    # Specify maximum number of epochs to process in imdb weight training:\n",
    "    imdb_epoch_num = 3\n",
    "    for idx, path in enumerate(tpaths):\n",
    "        print 'Training imdb-regression weights on training set %s' % (idx + 1)\n",
    "        train_idx = 't%s_' % (idx + 1)\n",
    "        weight_name_mod = imdb_weight_name_template + train_idx\n",
    "        for num in range(imdb_epoch_counts[idx] + 1, imdb_epoch_num):\n",
    "            print 'IMDb regression weight training on epoch:', num\n",
    "            imdb_reg.fit_generator(input_generator(path, output_vector_path, trans_paths[idx], \n",
    "                                                   transformed_data_dict[idx]), samples_per_epoch = transformed_len, \n",
    "                                                   nb_epoch=1)\n",
    "            fresh_imdb_weight_name = imdb_folders[idx] + weight_name_mod + str(num) + imdb_weight_name_ext\n",
    "            imdb_reg.save_weights(fresh_imdb_weight_name)\n",
    "        print 'Weight generation for imdb-regression complete on training set %s\\n' % (idx + 1)\n",
    "else:\n",
    "    print 'No further imdb-regression weight generation required'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many epochs have been processed:\n",
    "imdb_epoch_counts = epoch_mult_check(imdb_folders, imdb_weight_name_template, weight_name_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create references to imdb training weight files:\n",
    "print 'Trained imdb weight files\\' names:'\n",
    "imdb_trained_weights = []\n",
    "for idx, epnum in enumerate(imdb_epoch_counts):\n",
    "    ep_identifier = 't%s_%s' % (idx + 1, epnum)\n",
    "    file_name = imdb_folders[idx] + imdb_weight_name_template + ep_identifier + imdb_weight_name_ext\n",
    "    print file_name\n",
    "    imdb_trained_weights.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for imdb predictions:\n",
    "pred_folder_name = 'movie_predictions/'\n",
    "pred_folder = data_path + pred_folder_name\n",
    "if not os.path.exists(pred_folder):\n",
    "    print 'Creating movie predictions folder...'\n",
    "    os.mkdir(pred_folder)\n",
    "else:\n",
    "    print 'Movie predictions folder already exists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define movie predictions' names\n",
    "print 'Movie predictions file names:'\n",
    "pred_names = []\n",
    "movie_pred_name = 'movie_prediction_'\n",
    "for i in range(len(imdb_epoch_counts)):\n",
    "    pred = pred_folder + movie_pred_name + str(i + 1) + '.csv'\n",
    "    print pred\n",
    "    pred_names.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 'test index' file and 'output data' file:\n",
    "with open(test_name_path, 'rb') as test_set:\n",
    "    test_data = pkl.load(test_set)\n",
    "with open(output_vector_path, 'rb') as output_set:\n",
    "    output_data = pkl.load(output_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if regression == 1:\n",
    "    for idx, fx in enumerate(imdb_trained_weights):\n",
    "        # by_name allows for old weight files to be used with new models with new structures\n",
    "        imdb_reg.load_weights(fx, by_name=True)\n",
    "        # Load transformed data\n",
    "        with h5py.File(trans_paths[idx], 'r') as trans_open:\n",
    "            transformed_data = trans_open[transformed_data_dict[idx]][:]\n",
    "        # Run predictions on test data:\n",
    "        print 'Running predictions on test data with training set %s weights...' % (idx + 1)\n",
    "        prediction_vec = []\n",
    "        actual_score_vec = []\n",
    "        for index in test_data:\n",
    "            # Pull necessary data using index in test data:\n",
    "            test_row = transformed_data[index]\n",
    "            score_val = output_data[index]\n",
    "            # Populate placeholder vector:\n",
    "            placeholder_input = np.zeros((1, trans_row_len))  # 10000 placeholder\n",
    "            placeholder_input[0] = test_row\n",
    "            # Get prediction\n",
    "            imdb_predict = imdb_reg.predict(placeholder_input)\n",
    "            prediction_vec.append(imdb_predict)\n",
    "            actual_score_vec.append(score_val)\n",
    "        print 'Predictions have completed for training set %s weights! Proceeding to save data...' % (idx + 1)\n",
    "        final_results = pd.DataFrame()\n",
    "        final_results['Real_Score'] = actual_score_vec\n",
    "        final_results['Predicted_Score'] = prediction_vec\n",
    "        final_results.to_csv(pred_names[idx])\n",
    "        print 'Prediction data for dataset %s has been saved!\\n' % (idx + 1)\n",
    "else:\n",
    "    print 'Predictions for IMDb scores have already been run!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
