{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Collector for IMDb Predictor Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to extract data from a CSV file containing information on approximately 5000 movies, clean the extracted data, and format it for input into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries:\n",
    "from gensim.models import Word2Vec  # Latent semantic analysis package\n",
    "from os.path import exists as ex\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract index of desired fields:\n",
    "def target_field(master_list, desired_list):\n",
    "    desired_index = []\n",
    "    for elem in desired_list:\n",
    "        desired_index.append(master_list.index(elem))\n",
    "    return desired_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate keys for transforming data frame vectors:\n",
    "def key_create(col_df):\n",
    "    col_list = sorted(list(set(col_df)))\n",
    "    return col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to break up strings with pipes\n",
    "def break_pipe(string_pipe):\n",
    "    broken_line = string_pipe.split('|')\n",
    "    return broken_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace pipes with spaces\n",
    "def replace_pipe(string_pipe):\n",
    "    edit_line = string_pipe.replace('|', ' ')\n",
    "    return edit_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate binary vectors:\n",
    "def binary_vector_generate(val, lookup_key, ind):\n",
    "    lu_elem = lookup_key[ind]\n",
    "    vec_gen = np.zeros(len(lu_elem), dtype=np.int8)\n",
    "    if type(val) == list:\n",
    "        for val_elem in val:\n",
    "            vec_gen[lu_elem.index(val_elem)] = 1\n",
    "    else:\n",
    "        vec_gen[lu_elem.index(val)] = 1\n",
    "    return vec_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize inputs to 0-100 scale\n",
    "def normalize_value_generate(val, lookup_key, ind):\n",
    "    lu_elem = lookup_key[ind]\n",
    "    elem_min = min(lu_elem)\n",
    "    lu_elem[:] = [x - elem_min for x in lu_elem]\n",
    "    elem_shifted_max = max(lu_elem)\n",
    "    norm_val = (float(val-elem_min)/float(elem_shifted_max))*100\n",
    "    return norm_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate neural network input:\n",
    "def vector_gen(df_row, key_list_df, longest_len, vector_size, word_model, desired_fields):\n",
    "    \n",
    "    input_list = []\n",
    "    genre_index = desired_fields.index('genres')\n",
    "    keyword_index = desired_fields.index('plot_keywords')\n",
    "    \n",
    "    binary_names = ['color', 'director_name', 'actor_2_name', 'actor_1_name', 'actor_3_name', 'country', 'content_rating',\n",
    "                    'title_year', 'aspect_ratio']\n",
    "    binary_list = target_field(desired_fields, binary_names)\n",
    "    \n",
    "    norm_names = ['num_critic_for_reviews', 'duration', 'gross', 'budget']\n",
    "    norm_list = target_field(desired_fields, norm_names)\n",
    "    \n",
    "    # Iterate across elements in a row:\n",
    "    for ind, element in enumerate(df_row):\n",
    "        if ind in binary_list:\n",
    "            # Generate binary vector\n",
    "            input_list.append(binary_vector_generate(element, key_list_df, ind))\n",
    "        elif ind == genre_index:\n",
    "            # Special treatment to split words by pipes\n",
    "            word_list = break_pipe(element)\n",
    "            input_list.append(binary_vector_generate(word_list, key_list_df, ind))\n",
    "        elif ind == keyword_index:\n",
    "            # Special treatment using word embedding\n",
    "            vsize = longest_len*vector_size\n",
    "            word_vec = np.zeros(vsize)\n",
    "            embed_words = replace_pipe(element)\n",
    "            embed_words = embed_words.split()\n",
    "            for index, word in enumerate(embed_words):\n",
    "                word_vec_temp = word_model[word]\n",
    "                word_vec[(index*vector_size):((index+1)*vector_size)] = word_vec_temp\n",
    "            input_list.append(word_vec)\n",
    "        elif ind in norm_list:\n",
    "            # Normalize value\n",
    "            norm_val = [normalize_value_generate(element, key_list_df, ind)]\n",
    "            input_list.append(norm_val)\n",
    "    # Remove boundaries between individual elements in input list\n",
    "    # Before boundary removal, each attribute has an entry in the list and we need a straight aggregation\n",
    "    final_vector = []\n",
    "    for entry in input_list:\n",
    "        final_vector = final_vector + list(entry)\n",
    "    return final_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parent path of working directory:\n",
    "parent_path = '/Users/cheng-haotai/Documents/Projects_Data/IMDb_Predictor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of predictions folder (for later analysis):\n",
    "prediction_path = parent_path + 'prediction_results/'\n",
    "if not os.path.exists(prediction_path):\n",
    "    print 'Creating folder for storing prediction values...'\n",
    "    os.mkdir(prediction_path)\n",
    "else:\n",
    "    print 'Prediction values folder already exists!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set csv file path parameters:\n",
    "csv_name = 'movie_metadata.csv'\n",
    "csv_path = parent_path + csv_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model save path for Word2Vec model\n",
    "model_name = 'word_model.bin'\n",
    "model_folder = 'training_data/'\n",
    "model_path = parent_path + model_folder + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into pandas dataframe:\n",
    "imdb_df = pd.read_csv(csv_path)\n",
    "print 'Type of imdb_df:', type(imdb_df)\n",
    "print 'Size of data frame:', imdb_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefine expected column data types:\n",
    "dtypes_dict = {'color': str, 'director_name': str, 'num_critic_for_reviews': np.int, 'duration': np.int,\n",
    "               'director_facebook_likes': np.int, 'actor_3_facebook_likes': np.int, 'actor_2_name': str,\n",
    "               'actor_1_facebook_likes': np.int, 'gross': np.int, 'genres': str, 'actor_1_name': str,\n",
    "               'movie_title': str, 'num_voted_users': np.int, 'cast_total_facebook_likes': np.int, 'actor_3_name': str,\n",
    "               'facenumber_in_poster': np.int, 'plot_keywords': str, 'movie_imdb_link': str,\n",
    "               'num_user_for_reviews': np.int, 'language': str, 'country': str, 'content_rating': str, 'budget': np.int,\n",
    "               'title_year': np.int, 'actor_2_facebook_likes': np.int, 'imdb_score': np.float, 'aspect_ratio': np.float,\n",
    "               'movie_facebook_likes': np.int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data frame of rows with null values:\n",
    "print 'Column with most null values has %s missing values' % (imdb_df.isnull().sum().max())\n",
    "print 'Removing all rows with any missing values...'\n",
    "imdb_df.dropna(how='any', inplace=True)\n",
    "print 'Shape of reduced data frame:', imdb_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data types for columns in data frame:\n",
    "for ent in dtypes_dict:\n",
    "    imdb_df[str(ent)] = imdb_df[str(ent)].astype(dtypes_dict[ent])\n",
    "# Clean up string fields of escape characters:\n",
    "str_df = imdb_df.select_dtypes(include=['object'])\n",
    "for col in str_df.columns.values:\n",
    "    imdb_df[str(col)] = imdb_df[str(col)].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data frame to only desired fields:\n",
    "desired_fields = ['color', 'director_name', 'num_critic_for_reviews', 'duration', 'actor_2_name', 'gross', 'genres',\n",
    "                  'actor_1_name', 'actor_3_name', 'plot_keywords', 'country', 'content_rating', 'budget', 'title_year',\n",
    "                  'imdb_score', 'aspect_ratio']\n",
    "# Make new data frame that is a subset of imdb_df:\n",
    "work_df = imdb_df[desired_fields].copy(deep=True)\n",
    "print 'Shape of work data frame:', work_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general strategy for generating inputs for the neural network will be to either binarize (create vector of 0's and 1's with 1's indicating the presence of an entity) or normalize the values along each row. In the special case of plot keywords, word embedding using Word2Vec will be used to transform the keywords into usable input entities.\n",
    "\n",
    "Word2Vec produces a \"word vector\" using deep learning. Using phrases, Word2Vec generates a model that embeds context information from words / placement of words.\n",
    "\n",
    "Below, I will generate a \"unique\" list of each feature's elements for the purpose of creating either binary vectors or normalized values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transformation keys:\n",
    "key_list = []\n",
    "# Actor names must be aggregated for key generation:\n",
    "actor_index_name = ['actor_2_name', 'actor_1_name', 'actor_3_name']\n",
    "actor_index = target_field(desired_fields, actor_index_name)\n",
    "print 'Column index of actor names:', actor_index\n",
    "actor_agg = []\n",
    "\n",
    "# Piped strings need to be separated:\n",
    "genre_index = desired_fields.index('genres')\n",
    "keyword_index = desired_fields.index('plot_keywords')\n",
    "print 'Index of genre:', genre_index\n",
    "print 'Index of plot keywords:', keyword_index\n",
    "genre_agg = []\n",
    "keyword_agg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate aggregated list of unique values by iterating through columns of data frame\n",
    "for idx, col in enumerate(desired_fields):\n",
    "    df_col = work_df[str(col)]\n",
    "    # Checking for actor name\n",
    "    if idx in actor_index:\n",
    "        act_temp = key_create(df_col)\n",
    "        actor_agg += act_temp\n",
    "        key_list.append('ACTOR')\n",
    "    # Checking for genre\n",
    "    elif idx == genre_index:\n",
    "        for phrase in df_col:\n",
    "            phrase_temp = break_pipe(phrase)\n",
    "            genre_agg += phrase_temp\n",
    "        key_list.append('GENRE')\n",
    "    # Checking for plot keyword\n",
    "    elif idx == keyword_index:\n",
    "        for phrase in df_col:\n",
    "            phrase_temp = replace_pipe(phrase)\n",
    "            phrase_temp = phrase_temp.split()\n",
    "            keyword_agg.append(phrase_temp)\n",
    "        key_list.append('KEYWORD')\n",
    "    else:\n",
    "        key_temp = key_create(df_col)\n",
    "        key_list.append(key_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keys from aggregated lists\n",
    "act_key = sorted(list(set(actor_agg)))\n",
    "genre_key = sorted(list(set(genre_agg)))\n",
    "keyword_key = sorted(keyword_agg)\n",
    "# Replace placeholder names in list of keys with aggregated keys\n",
    "for idx, elem in enumerate(key_list):\n",
    "    if elem == 'ACTOR':\n",
    "        key_list[idx] = act_key\n",
    "    elif elem == 'GENRE':\n",
    "        key_list[idx] = genre_key\n",
    "    elif elem == 'KEYWORD':\n",
    "        key_list[idx] = keyword_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of words in the longest phrase in plot keyword list to standardize input length for keywords:\n",
    "phrase_len = []\n",
    "for key_phrase in keyword_key:\n",
    "    phrase_len.append(len(key_phrase))\n",
    "longest_len = max(phrase_len)\n",
    "print 'Longest phrase in plot keyword has %s number of words' % (longest_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for storing data:\n",
    "model_loc = parent_path + model_folder\n",
    "if not ex(model_loc):\n",
    "    print 'Generating folder for storing training, test, and validation data...'\n",
    "    os.mkdir(model_loc)\n",
    "    print 'Data folder successfully generated!'\n",
    "else:\n",
    "    print 'Data folder already exists!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a model using Word2Vec to capture relative \"closeness\" of phrases in plot keyword\n",
    "# Specify parameters for Word2Vec\n",
    "vector_size = 300\n",
    "min_count = 1\n",
    "alpha = 0.025\n",
    "if not ex(model_path):\n",
    "    print 'Training Word2Vec model...'\n",
    "    word_model = Word2Vec(keyword_key, size=vector_size, min_count=min_count, alpha=alpha, hs=1, negative=0)\n",
    "    word_model.save(model_path)\n",
    "    print 'Word2Vec model finished generating!'\n",
    "else:\n",
    "    print 'Loading trained model...'\n",
    "    word_model = Word2Vec.load(model_path)\n",
    "    print 'Word2Vec model finished loading!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output vector:\n",
    "output_vector = list(work_df.imdb_score)\n",
    "output_vector_name = 'output_data.pickle'\n",
    "output_path = parent_path + model_folder + output_vector_name\n",
    "if not ex(output_path):\n",
    "    print('Saving output vector pickle file...')\n",
    "    with open(output_path, 'wb') as output_out:\n",
    "        pkl.dump(output_vector, output_out)\n",
    "    print('Output vector pickle file successfully saved!')\n",
    "else:\n",
    "    print('Output vector pickle file already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vector parameters:\n",
    "input_vector_name = 'input_vectors.h5'\n",
    "input_vector_path = parent_path + model_folder + input_vector_name\n",
    "input_dataset_name = 'input_dataset'\n",
    "\n",
    "if not ex(input_vector_path):\n",
    "    # Generate input vectors row-by-row:\n",
    "    input_vectors = []\n",
    "    print 'Assembling input vector...'\n",
    "    for idx, row in work_df.iterrows():\n",
    "        vector_temp = vector_gen(row, key_list, longest_len, vector_size, word_model, desired_fields)\n",
    "        input_vectors.append(vector_temp)\n",
    "    print 'Length of input vector is:', len(input_vectors)\n",
    "    print 'Length of element in input vector:', len(input_vectors[0])\n",
    "    # Save input vectors:\n",
    "    print 'Saving input vector h5 file...'\n",
    "    input_out = h5py.File(input_vector_path, 'w')\n",
    "    input_out.create_dataset(input_dataset_name, data=input_vectors)\n",
    "    input_out.close()\n",
    "    print 'Input vector h5 file successfully saved!'\n",
    "else:\n",
    "    print 'Input vector h5 file already exists!'\n",
    "    # Load data from h5 file:\n",
    "    print 'Loading input vector h5 file...'\n",
    "    with h5py.File(input_vector_path, 'r') as input_grab:\n",
    "        input_vectors = input_grab['input_dataset'][:]\n",
    "    print 'Successfully loaded input vector h5 file!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I've created an h5 file containing the transformed version of every single row of the original IMDb dataframe. The transformation has summarized each row into a series of numbers that are mixtures of binary vectors, normalized values, and word vectors containing context information.\n",
    "\n",
    "Now, we will proceed to split this master list into 3 separate training sets. Each of these training sets will be used to train a neural network model and the results of 3 separate neural networks will be averaged to get final prediction values.\n",
    "\n",
    "The split will be accomplished by defining indicies by which to reference the master-list of input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define master indices list:\n",
    "shuffled_indices = np.random.permutation(len(input_vectors))\n",
    "print 'Length of shuffled indices:', len(shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data set names:\n",
    "test_name = 'test_index.pickle'\n",
    "train1_name = 'train1_index.pickle'\n",
    "train2_name = 'train2_index.pickle'\n",
    "train3_name = 'train3_index.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data set paths:\n",
    "test_path = parent_path + model_folder + test_name\n",
    "predres_test_path = prediction_path + test_name\n",
    "train1_path = parent_path + model_folder + train1_name\n",
    "train2_path = parent_path + model_folder + train2_name\n",
    "train3_path = parent_path + model_folder + train3_name\n",
    "\n",
    "train_list = [train1_path, train2_path, train3_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data files already exist and generate if they don't exist:\n",
    "if not ex(test_path) or not ex(train1_path) or not ex(train2_path) or not ex(train3_path):\n",
    "    print 'Creating data files for training set indices and test set indices...'\n",
    "    \n",
    "    # Create and save test indicies:\n",
    "    test_num = int(np.ceil(0.15 * len(input_vectors)))\n",
    "    print 'Number of elements in test set is:', test_num\n",
    "    test_indices = shuffled_indices[-test_num:]\n",
    "    shuffled_indices = shuffled_indices[:-test_num]\n",
    "    # Save in training data folder\n",
    "    with open(test_path, 'wb') as test_out:\n",
    "        pkl.dump(test_indices, test_out)\n",
    "    # Save in predictions result folder\n",
    "    with open(predres_test_path, 'wb') as predres_out:\n",
    "        pkl.dump(test_indices, predres_out)\n",
    "    \n",
    "    # Create and save training indices\n",
    "    adjusted_len = len(shuffled_indices) - test_num\n",
    "    cutoff_fig = int(np.floor(adjusted_len*0.9))\n",
    "    print 'Number of elements in training sets is:', cutoff_fig\n",
    "    # Iterate through list of training set names:\n",
    "    for name in train_list:\n",
    "        new_shuffled = np.random.permutation(shuffled_indices)\n",
    "        model_test_indices = new_shuffled[:cutoff_fig]\n",
    "        with open(name, 'wb') as train_out:\n",
    "            pkl.dump(model_test_indices, train_out)\n",
    "    print 'All training set indices and test set indicies have been saved!'\n",
    "else:\n",
    "    print 'Data files for training set indices and test set indices already exist!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
