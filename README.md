# IMDb Predictor
Welcome to my personal project! The IMDb Predictor is a neural network that uses deep learning to be able to predict movies' IMDb scores based on a variety of factors (i.e. budget, actor names, director names, facebook likes, etc...). The IMDb Predictor neural network features a **multilayer perceptron** for generating score predictions and an **auto-encoder** to perform dimensionality reduction on the training datasets.

Please continue reading below to find required software packages and instructions for the correct order in which to run the scripts.

![Image of movie reel](https://github.com/gestalt-howard/IMDb_Predictor/blob/master/images/Movie-Tavern-Blog-Hero-Image.jpg)

***DISCLAIMER 1: I do not own this dataset. The original dataset was pulled from: https://www.kaggle.com/tmdb/tmdb-movie-metadata in early 2017. The dataset has since changed and WILL NOT be compatible with the scripts in this repo.***

## Software Prerequisites:
* python (2.7.14 required)
* pip install numpy
* pip install pandas
* pip install gensim
* pip install keras
* pip install h5py
* pip install jupyter

## Hardware Prerequisites:
**IMPORTANT:** IMDb Predictor requires a very powerful GPU to train the auto-encoder and multilayer perceptron. The recommended GPU is ***Tesla V100 PCIe***. Using AWS EC2 is also an option although be aware of the fast-accruing costs!

It is also possible to run through IMDb Predictor using a much-reduced dataset. The inclusion of *movie_metadata_debug* in this repo is for this exact purpose. You can get a good understanding of how these scripts are meant to work using this dataset.

## Data Prerequisites:
* CSV formatted with label names as first row and columns of values
	* Advised Dataset- https://github.com/gestalt-howard/IMDb_Predictor

## Pipeline Overview:
I recommend using Jupyter's **Restart and Run All** function accessed in the ***Kernel*** tab on the top toolbar.
1. Run through the ***imdb_input_collector.ipynb*** notebook to generate data required for training the neural network
2. Run through the ***imdb_training_script.ipynb*** notebook to train the neural network and output score predictions
3. Run through the ***imdb_analysis.ipynb*** to visualize and assess the neural network's performance. *(Note: The file shown in this repo already contains the results of my own personal analysis.)*

## Detailed Pipeline Order:
### 1. Clean Data and Format for Neural Network:
Use *imdb_input_collector.ipynb*

#### 1.1 Actions Taken:
1. **User Action Required:** Ensure that all target file paths (i.e. file name, folder directory structure, etc...) are correct
	* **IMPORTANT:** If you'd like to run through the scripts in debug mode, set the `debug_flag` at the top of the Jupyter script to `0`
	* The remaining scripts are dependent on this initial `debug_flag` value
2. Takes CSV file contents and cleans the data of rows with null values
3. Takes subset of remaining dataset and formats fields into binary vectors, normalized values, and word vector with embedded context information
	* Word vector embedding performed using Word2Vector from the gensim library

#### 1.2 Folders and Files Generated:
**NOTE:** The three training sets are used for generating three different regression models. The predictions generated form these three regression models will be averaged to yield greater stability.
* ***training_data (folder)***
	* **word_model.bin**
		* Binary file generated by Word2Vector
		* Model that contains contextual data from text phrases
	* **train1_index.pickle**
		* Contains the row indexes that reference the rows of the *input_vectors.h5* to create the first training set
	* **train2_index.pickle**
		* Same functionality as *train1_index.pickle* but for the second training set
	* **train3_index.pickle**
		* Same functionality as *train1_index.pickle* but for the third training set
	* **test_index.pickle**
		* Same functionality as *train1_index.pickle* but for the test set
		* The test set contains indexes that reference the rows of *input_vectors.h5* that construct the test set
	* **output_data.pickle**
		* Contains the actual IMDb scores that correspond to the rows of the *input_vectors.h5* file
	* **input_vectors.h5**
		* Contains rows of transformed information
		* Text and numerical data have been transformed and normalized into a format suitable for feeding into a neural network
* ***prediction_results (folder)***
	* **test_index.pickle**
		* Same file as in the *training_data* folder
		* This occurrence is used for analysis of the prediction results
	* **train1_index.pickle**
		* Same file as in the *training_data* folder
		* Used for analysis of the prediction results
	* **train2_index.pickle**
		* Same as above
	* **train3_index.pickle**
		* Same as above

### 2. Train the Neural Networks and Predict Results:
Use *imdb_training_script.ipynb*

#### 2.1 Actions Taken:
1. **User Action Required:** Ensure that all target file paths (i.e. file name, folder directory structure, etc...) are correct
2. Set the flags to either `0` or `1` depending on which segment of the code you wish to run. There are 4 distinct segments:
	* ***test_ae***: begin or continue generating weight files for a specified number of epochs (autoencoder model)
	* ***ae***: load latest epoch's weights and transform data in dimensionality
	* ***test_regression***: begin or continue generating weight files for a specified number of epochs **for each training set** (IMDb regression model)
	* ***regression***: load latest weight file for IMDb regression and use weights to predict IMDb scores from the test set data
3. Default settings are `1` for all values which will run through the entire code

#### 2.2 Folders and Files Generated:

* ***training_data/autoen_training_weights (folder)***
	* **autoen_weights_0.h5**
	* **autoen_weights_1.h5**
	* **autoen_weights_2.h5**
* ***training_data/imdb_training_data (folder)***
	* ***train1 (folder)***
		* **imdb_weights_t1_0.h5**
		* **imdb_weights_t1_0.h5**
		* **imdb_weights_t1_0.h5**
	* Additional folders ***train2*** and ***train3*** contain similar weight files
* ***training_data/transformed_data (folder)***
	* **transformed_data.h5**
		* This file contains the reduced-dimensionality data that is the result of feeding the rows of *input_vectors.h5* into the autoencoder neural network
* ***prediction_results (folder)***
	* **movie_prediction_1.csv**
		* Contains the predicted and actual IMDb scores generated from the regression model trained from training set 1
	* **movie_prediction_2.csv**
	* **movie_prediction_3.csv**
	* **test_index.pickle**

### 3. Analysis is Performed:
Use *imdb_analysis.ipynb*

#### 3.1 Actions Taken:
1. **User Action Required:** Ensure that all target file paths (i.e. file name, folder directory structure, etc...) are correct
2. Reformat the prediction results and import the results into a pandas dataframe
3. Calculate average prediction results and basic statistical values
4. Visualize the prediction results and assess the performance of the neural network by conducting null hypothesis testing, plotting Gaussians, and looking at outliers

#### 3.2 Folders and Files Generated:
* ***prediction_results/formatted_predictions (folder)***
	* **formatted_1.csv**
		* Contains formatted prediction results for training set 1
	* **formatted_2.csv**
		* Same as above for training set 2
	* **formatted_3.csv**
		* Same as above for training set 3
